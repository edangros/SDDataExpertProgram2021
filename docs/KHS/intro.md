# Kafka, Hadoop, Spark

아파치의 3대 빅데이터 프레임워크.


## 빅데이터의 3대 요소

우선적으로 위 3가지 솔루션으로 처리해야 할 '빅데이터'가 무엇인지를 알아야 한다. 아래의 두 가지 정의가 보편적으로 사용된다.
* **데이터의 규모**가 너무 커 기존 데이터베이스 관리도구의 데이터 수집/저장/관리/분석 역량으로는 처리가 불가능한 데이터 (맥킨지)
* 기존과 다른 **업무 수행 방식**을 통해 *다양한 종류*의 *대규모* 데이터로부터 저렴한 비용으로 가치를 추출하고, 데이터의 빠른 수집/저장/발굴/분석을 지원하도록 고안된 차세대 기술과 아기텍쳐(IDC)


빅데이터의 3대 요소는 아래의 3가지(3V) 를 보통 말한다. 이 중 두가지 이상의 요소가 충족되면 빅데이터라 볼 수 있다.

### 크기(Volume)
* 수십 TB, 또는 PB 이상의 데이터
* 기존 파일 시스템으로는 저장조차 버거운 크기로, 분석은 더더욱 어려움
* 분산된 클러스터(Cluster) 형태의 처리장치가 요구되며, 클라우드(Cloud) 시스템과 결합되기도 함
* 구글의 GFS, 아파치의 Hadoop 등

### 속도(Velocity)
* 실시간 처리 : 데이터가 쌓이는 속도가 매우 빠르기 때문에 수집/저장/분석을 실시간으로 할 수 있어야 함
* 장기적 접근 : 그와 동시에 다양한 분석 기법과 표현을 위해 장기적인 접근 또한 필요함
* 자연어 처리, 패턴 인식, 데이터 마이닝, 머신러닝 등 대용량의 비정형 데이터를 효과적으로 처리하기 위한 기법 필요

### 다양성(Variety)
* 빅데이터는 비정형 데이터를 처리할 수 있어야 함
* 정형(고정된 필드에 저장되는 일정한 형식, 즉 SQL에 저장가능한 형식)
* 반정형(필드는 고정되지 않지만 어느정도의 스키마를 따라가는 형식. HTML이나 XML 등)
* **비정형 데이터는 고정된 필드가 정의될 수 없음. 사진, 동영상, 포스팅이나 채팅 내용 등**
  * (여기서 채팅 데이터 자체는 정형 데이터로 처리가 가능하지만, *채팅으로 친 내용*은 비정형임에 유의)
  
  
## Kafka, Hadoop, Spark의 전체적인 아키텍쳐
아래 내용은 [Jayesh Nazre의 2017년 블로그 포스트](https://pmtechfusion.blogspot.com/2017/11/big-datadata-science-analytics-apache_20.html)에 기반하고 있으며
그동안 프레임워크에 일어난 변화를 반영하기 위해 다음 내용을 참고하였음
* [Apache Kafka 공식 문서](https://kafka.apache.org/documentation/)
* [그로윈 하둡 제품 설명](https://www.growin.co.kr/hadoop)

