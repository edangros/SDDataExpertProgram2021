---
layout: post
title: "Hadoop"
date: 2021-01-07 14:22:21 UTC+9
comments: false
---

# Hadoop
하둡의 공식 지원문서는 상당히 빈약한 축이나, 장기간의 빅데이터 분석의 주류를 차지한 덕분에 제3자의 설명들이 자세히 만들어져 있다. 이 문서는 [intro](intro.md)의 내용과 더불어 아래 글들을 참조하였다.

* [아파치 공식 하둡 문서](https://hadoop.apache.org/docs/stable/) 및 [하둡 홈페이지](https://hadoop.apache.org/)
* [How do Hadoop and Spark Stack Up?](https://logz.io/blog/hadoop-vs-spark/) - logz.io
* [내가 아는 모든 IT ~ > 하둡 정보](https://opentutorials.org/course/2908/17055) - Opentutorials.org
* [강동현님의 Hadoop/Spark 강의자료](https://www.slideshare.net/KangDognhyun/apache-spark-70360736) - CC-attribution 라이센스
* [Apache Hive Essentials 2nd Edition](https://subscription.packtpub.com/book/application_development/9781788995092/) - packt, Overview 부분
<br><br>

##  1. <a name=''></a>하둡 생태계

![하둡 생태계](https://static.packt-cdn.com/products/9781788995092/graphics/c8625da0-2ffb-41b7-bba8-58c33af68a30.png)

하둡의 핵심을 이루는 세 가지 요소는 아래와 같다. 하둡과 Spark의 연산방식 차이를 가장 크게 가르는 **인메모리 처리** 부분에 [스팍](spark.md)이 언급된 부분을 주목할 만 하다.

[인트로](intro.md)에서 언급하였듯이, 데이터의 크기가 커짐에 따라 전통적인 1pc, 1어플리케이션, 1개의 채널로 구동되는 데이터 처리는 더이상 스케일업(ScaleUp)으로 감당할 수 없는 수준에 도달하였다. 단순한 멀티프로세싱(multiprocessing)이나 멀티쓰레딩(multithreading)으로는 이러한 급격한 데이터 분량의 상승을 위해 극단적인 성능 상승이 필요해졌고, 각 머신의 저장 구조로는 데이터를 저장하는 것 자체부터 한계에 도달했다. 이에 데이터를 분산 저장하고, 분산해서 요청하며, 요청된 처리 내용을 분산된 노드들이 작은 단위로 쪼개어 처리하는 것(마이크로프로세싱,microprocessing)이 필요해진 것이다.

하둡(Hadoop)은 아파치 소프트웨어 재단(Apache Software Foundation)의 최상위 프로젝트로, '신뢰할 수 있고', '스케일링할 수 있으며', '분산처리되는' 연산을 위해 만들어진 오픈소스 소프트웨어이다. 2013년 Hadoop2, 2016년 Hadoop3이 발표됨에 따라 성능이 지속적으로 향상되어오고 있다. 특히 Hadoop2 이후로는 YARN을 통해 클러스터를 종합적으로 관리할 수 있게 되고, 하둡 시작부터 함께한 MapReduce와 더불어 다른 종류의 데이터 프로세싱 모듈을 함께 사용할 수 있게 되었다.

##  2. <a name='3'></a>하둡의 기본 3요소
###  2.1. <a name='YARNYetAnotherResourceNegotiator'></a>YARN (Yet Another Resource Negotiator)
빅데이터를 분석하기 위해서는 분석 어플리케이션이 실행되어야 한다. 하지만 빅데이터를 한 어플리케이션이 맡아 분석하는 것에는 무리가 따르며, 클러스터를 구성하는 수많은 머신들이 분담해야 한다. 하둡 1에서는 오로지 MapReduce만이 사용가능했으며, MapReduce의 JobTracker 기능이 이를 맡았지만, 이제 관리에 해당하는 모든 기능은 YARN이 관리하고 있다.

![YARN 구조도](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/2926/6736.png)

* Resource Manager
    * 클러스터당 1개 존재하며 클러스터의 전반적인 자원 관리, 스케줄링, 어플리케이션 매니저를 조정
    * 클라이언트로부터 어플리케이션 실행 요청을 받으면 그 실행을 책임지는 Application Master를 실행
    * 클러스터 내의 각 Node Manager들과 통신, 할당된 자원과 사용중인 자원의 상황을 알 수 있음
    * 노드로 구성된 *하나의 팀*인 클러스터를 지휘, 클러스터 내부에서 현재 사용중인 자원과 사용 가능한 자원들을 파악하고, 새로운 요청을 처리하는 역할을 맡음
* Scheduler
    * Node Manager들의 자원 상태를 관리하며, 부족한 리소스를 배분한다.
    * 자원이 부족하다면 Task를 지연시키는 등, 스케쥴링 작업을 관할한다.
    * Node Manager들의 자원 상태는 Resource Manager가 직접 통신하지는 않고, 스케쥴러가 통지하는 구조이다.
* Application Manager
    * 노드 내에서 특정 작업이 요구된다면, Node Manager에서 Application Manager를 실행하고, Application의 실행 상태를 Resource Manager에게 통지한다.
* Node Manager
    * 하나의 노드를 관할하는 노드당 하나 존재하는 관리자이다.
    * 각 컨테이너의 리소스 사용량을 모니터링하며 관련 정보를 Resource Manager에게 알린다.
* Application Master
    * YARN상에서 실행되는 하나의 태스크를 관리하는 마스터 서버
    * 실행되는 어플리케이션당 1개만 존재
    * Scheduler로부터 적절한 컨테이너를 할당받은 뒤, 어플리케이션의 실행 상태를 모니터링하고 관리한다.
* Container
    * 작업이 수해오디는 실질적인 공간이다.
    * 모든 작업은 여러 태스크로 분리되며, 각 태스크는 하나의 Container에서 실행된다.

위 아키텍쳐를 기반으로 YARN은 아래 프로세스에 따라 작동한다.

![YARN 작동 개요도](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/2926/6738.png)

1. Resource Manager가 YARN 클라이언트가 제출하는 작업(Job 또는 Application)을 수령
2. 마스터 노드의 Application Manager가 유효성을 확인한 뒤 스케쥴러에 자원 할당을 요구
3. 스케쥴러는 임의의 노드상의 컨테이너 하나를 Application Master에게 할당. **Application Master가 생성됨.**
4. Application Master는 노드상의 데이터 위치, 및 자원 등을 Resource Manager에게 제공하여 다른 컨테이너들을 조율할 수 있게 함
5. Resource Manager는 리소스를 할당하고, Application master에게 노드의 세부정보들을 전달함
6. Application master는 지정된 노드들의 Node manager에게 컨테이너 실행을 요청함
7. 작업(Job)이 실행되는 동안, 컨테이너의 리소스는 Application master가 관리하고, 작업 종료시 Resource manager에게 통지
8. Node manager는 스케쥴러가 새로운 application에 대해 작업을 시작할 수 있도록 주기적으로 사용 가능한 리소스의 상태를 Resource manager에게 통지
9. Slave node에서 이상 발생시, Resource manager는 application master에게 새로운 컨테이너를 할당, 이상이 발생한 노드 대신 사용하여 프로세스를 끝낼 수 있도록 함.
<br>
<br>
<br>

##  3. <a name='HDFSHadoopDistributedFileSystem'></a>HDFS (Hadoop Distributed FileSystem)
하둡 분산형 파일시스템은 하둡 네트워크에 연결된 데이터 클러스터에 데이터를 저장하고 관리하기 위한 파일시스템이다. 빅데이터는 단일 기기에는 저장조차 힘들기 때문에, 다중의 기기에 저장하면서도, 동시에 무결성을 확보하기 위한 주요 장치이다.

###  3.1. <a name='HDFS'></a>**HDFS의 기본 가정**
아파치에서는 몇 가지 가정을 한 뒤, 이를 통해 HDFS가 갖추어야 할 목표사항을 설정하였고, 달성하였다.

####  3.1.1. <a name='-1'></a>**하드웨어 고장**
하드웨어 고장은 예외적인 상황이 아니라 거의 항상 벌어지는 일이다. HDFS는 수백, 수천개의 물리 기기로 이루어질 수 있고, 이들 중 한둘은 항상 고장나있는 상태일 것이다. 따라서 HDFS는 일부 노드가 고장난 상태에서도 이를 빠르게 감지한 뒤, 자동으로 빠르게 복구해야 한다. 이는 **HDFS의 가장 중요한 목표이다**.

####  3.1.2. <a name='-1'></a>**데이터 스트리밍**
빅데이터는 매우 큰 만큼, HDFS에 클라이언트가 데이터를 요구할 때에는 스트리밍을 통해 전송받을 수 있어야 한다. HDFS는 빅데이터이며, 유저가 상호작용하며 데이터를 획득하는 대신, 일련의 배치(batch) 명령을 통해 데이터를 작업할 수 있도록 디자인되어있다. 따라서 HDFS은 처리 지연을 줄이는 대신 처리 용량을 키우는 방향으로 디자인되었으며, **즉시 처리에 강하지 않다!**

간략히 말하면, HDFS로부터 데이터를 한순간에, 짧은 시간 내에 받는 건 가능하지 않다. HDFS는 많은 양의 데이터를 계속 처리하기 위해 디자인되었으며, 데이터는 스트림을 통해 지속적으로 출력된다.

####  3.1.3. <a name='-1'></a>**거대한 데이터 세트**
HDFS에서 작동되는 어플리케이션은 거대한 데이터 세트를 가지고 있을 것이다. 따라서 HDFS는 거대한 파일들을 지원하도록 튜닝되어야 하며, 넓은 데이터 대역폭을 가져야 하고, 하나의 클러스터에 수백개의 노드를 가질 수 있어야 한다. 그로부터 수백, 수천만개 이상의 파일을 하나의 인스턴스에서 다룰 수 있게 될 것이다.

####  3.1.4. <a name='-1'></a>**간단한 의존성 모델**
HDFS 애플리케이션들은 한 번 작성된 파일을 다수가 여러번 액세스하는 구조이다. 따라서, 여러 어플리케이션에서 동시에 액세스되는 일이 빈번히 발생할 것이며, **편집은 엄격히 제한되어야 한다**. 기존의 HDFS는 이동, 삭제, 복사 외에 어떠한 수정도 허용하지 않았으나 최신 하둡에서는 파일 끝에서 내용을 연장하는 행위(append)는 허용된다.

이러한 방식으로 편집이 엄격히 제한된 결과, 데이터 사이의 의존성은 간단해지며, 데이터의 처리 용량은 커진다. (데이터 사이의 의존성이 약화되었으니 목표 데이터만 출력하면 된다) MapReduce 방식이나 웹 크롤러는 이러한 방식에 최적이다.

####  3.1.5. <a name='-1'></a>**연산장치를 옮기는 것이 데이터를 옮기는 것보다 저렴하다**
어플리케이션에 의해 요청된 연산은 데이터가 동작하는 지점과 가까운 곳에서 실행되고 있을 때 더 효율적이다. 특히 데이터셋이 거대해질수록 네트워크 정체가 커지며 처리 용량이 부족해지기에, 연산은 데이터와 가까운 곳에서 실행될 필요가 있다. 이 가정을 통해 연산은 어플리케이션보다는 데이터와 가까운 곳에서 실행되는 것이 바람직하며, HDFS는 데이터가 위치한 곳 근처에서 연산이 진행될 수 있도록 인터페이스를 제공한다.

####  3.1.6. <a name='-1'></a>**다양한 플랫폼과 하드웨어로의 이전 능력**
HDFS는 다양한 플랫폼 사이에서 옮겨다닐 수 있도록 설계되었다. 이를 통해 수많은 어플리케이션에서 HDFS를 간편하게 사용할 수 있다.
<br><br><br>
###  3.2. <a name='HDFS-1'></a>**HDFS 아키텍처**
![HDFS의 아키텍처](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/2926/6496.png)

* 파일을 저장할 때에는 한곳에 한 덩어리로 저장하지 않고 파일을 지정된 단위의 블록으로 나눈 뒤, 데이터노드들 사이에 분산하여 저장.
* 하나의 블록은 기본 3개(수정 가능)으로 복제되어 다른 노드에 저장한다. 이로써 한 노드가 손상되어도 다른 곳의 복제 데이터를 통해 빠르게 복구할 수 있다.
* HDFS의 네임노드는 마스터 역할을 하며, HDFS상에 하나 존재한다. 네임노드는 HDFS의 메타데이터(블록이 저장되는 위치, 파일명 등)를 관리하고, 클라이언트의 요청에 따라 HDFS의 파일에 접근할 수 있게 한다.
* 하둡 어플리케이션은 HDFS에 파일을 저장하거나 읽기 위해 HDFS 클라이언트를 사용하며, 클라이언트는 API의 형태로 제공된다.
* 데이터 노드는 주기적으로 네임노드에서 블록 리포트(노드에 저장된 블록의 정보)를 전송하고, 네임노드는 그로부터 데이터 노드의 정상 동작 여부를 확인할 수 있다.
* HDFS 클라이언트는 네임노드에 접속해 원하는 파일의 위치를 확인한 뒤, 해당 블록이 저장된 데이터노드로부터 직접 데이터를 가져올 수 있다.

###  3.3. <a name='HDFS-1'></a>**HDFS상 파일 저장**
![HDFS상의 파일 저장](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/2926/6497.png)
1. 어플리케이션이 HDFS 클라이언트를 사용하여 네임노드에게 파일 블록들을 저장할 경로를 생성해달라는 요청을 보낸다. 네임노드는 아래 작업을 시행한다.
    * 파일 경로가 존재하지 않으면 경로를 생성한다.
    * 다른 클라이언트에 의한 동시 수정이 일어나지 않도록 락을 건다.
    * HDFS 클라이언트에게 파일 블록들을 저장할 데이터노드 목록을 반환한다.
2. 클라이언트는 첫 데이터 노드에게 데이터를 전송한다.
3. 데이터 노드는 데이터를 로컬에 저장한 뒤 두 번째 데이터 노드로 전송한다.
4. 두 번째 데이터 노드도 마찬가지로 로컬에 저장한 뒤 다음 데이터 노드로 전송한다.
5. 여기서는 데이터가 3개로 복제된다. 세 번째 데이터 노드도 전송받은 데이터를 로컬에 저장한 뒤, 자신에게 데이터를 전송한 노드에 저장 완료 응답을 보낸다.
6. 마찬가지로 저장 완료 응답을 앞쪽 데이터 노드로 전송한다.
7. 첫 번째 데이터 노드는 클라이언트에 저장 완료 응답을 보낸다.
이 과정에서 저장 완료 응답이 도달했는지 여부를 통해 데이터노드의 손상 여부도 파악할 수 있다.

###  3.4. <a name='HDFS-1'></a>**HDFS상 파일 읽기**
![HDFS상의 파일 읽기](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/2926/6498.png)
1. 어플리케이션은 HDFS 클라이언트에게 파일 읽기를 요청한다.
2. HDFS 클라이언트는 네임노드에 요청된 파일의 블록 리스트를 요청
3. 네임노드는 요청된 파일이 저장된 블록 리스트를 반환
4. 해당 리스트를 바탕으로 클라이언트는 데이터노드에 블록 조회 요청
5. 데이터노드는 클라이언트에게 요청된 블록을 전송
6. 클라이언트는 어플리케이션에 데이터 전달

<br><br><br>

##  4. <a name='MapReduce'></a>MapReduce
대용량의 데이터 처리를 위한 분산 프로그래밍 모델 및 소프트웨어 프레임워크로, 하둡에서 분산 컴퓨팅 환경을 사용, 데이터의 병렬분석을 수행하기 위해 만들어졌다. 하둡의 시작부터 만들어진 기본 분석 툴로, 프로그래머가 직접 작성하는 **맵**과 **리듀스**, 두 개의 메소드로 구성되어있다.
* 맵 : 흩어져 있는 데이터를 연관성으로 분류하는 작업. (key, value) 형태. (자바/파이썬/C#의 '딕셔너리'가 '해시맵'임을 기억하자!)
* 리듀스 : Map에서 출력된 데이터에서 중복데이터를 제거하고 원하는 데이터를 추출하는 과정

예로, 문자열 내에서 단어의 빈도수는 아래와 같은 과정을 통해 수행할 수 있다.

![MapReduce 예시](https://s3-ap-northeast-2.amazonaws.com/opentutorials-user-file/module/2926/6506.png)

1. 쪼개기(Splitting) : 문자열 데이터를 쪼갠다.(본 예시에서는 라인별로 나누었다)
2. 맵핑(Mapping) : 맵을 만든다. 여기서는 단어와 출현횟수(1회)로 만들었다.
3. 셔플링(Shuffling) : 같은 key의 데이터끼리 모은다.
4. 리듀스(Reducing) : 같은 key의 데이터끼리 합쳐 수를 줄인다(Reduce). 여기서는 출현횟수를 합쳐 빈도수로 만든다.
5. 최종 결과(Final Result) : 결과 데이터를 저장한다. 하둡 시스템에 다시 저장할 수도 있고 다른 저장소를 사용할 수도 있을 것이다.

###  4.1. <a name='Job'></a>**잡(Job)**
클라이언트가 수행하려는 작업 단위를 의미한다. 해당 잡은 YARN에 의해 다수의 노드에서 실행되게 될 것이다.

이러한 작업 단위는 입력데이터, 맵리듀스 프로그램, 설정 정보로 구성된다.

인터넷상에는 구버전의 하둡 설명이 많지만, 현 시점에서 맵리듀스는 JobTracker와 TaskTracker를 사용할 필요가 없다. 해당 기능은 YARN이 수행하게 된다. JobTracker의 기능은 Resource Manager와 Application Master가 수행한다. Resource Manager는 현재 클러스터의 리소스 상태를 반환할 수 있으며, Application Master는 Resource Manager와 소통하며 실재로 작동하는 Job 전체를 관할한다.

TaskTracker는 Node Manager가 관할한다. 노드 내부의 컨테이너에서 실질적인 MapReduce 작업이 작동하며, Node Manager는 각 노드 내부의 컨테이너들을 관할하고 Resource Manager에게 자신의 자원 현황을 보고하게 된다.